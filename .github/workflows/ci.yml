---
name: CI/CD Pipeline

"on":
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  build-and-test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        dotnet-version: ["8.0.x"]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup .NET
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: ${{ matrix.dotnet-version }}

      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: ~/.nuget/packages
          key: ${{ runner.os }}-nuget-${{ hashFiles('**/packages.lock.json') }}
          restore-keys: |
            ${{ runner.os }}-nuget-

      - name: Bootstrap dependencies
        run: ./script/bootstrap

      - name: Lint code
        run: ./script/lint --annotations

      - name: Build project
        run: ./script/build

      - name: Run tests with coverage
        run: ./script/test --coverage

      - name: Run security checks
        run: ./script/security

      - name: Upload coverage to GitHub
        uses: actions/upload-artifact@v4
        with:
          name: coverage-report
          path: TestResults/CoverageReport
          retention-days: 30

  script-tests:
    runs-on: ubuntu-latest
    name: Script Tests

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup .NET
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: "8.0.x"

      - name: Test core scripts
        run: |
          if ! ./script/test-scripts; then
            echo ""
            echo "❌ Script tests failed!"
            echo ""
            echo "To fix issues locally:"
            echo "  1. Run: ./script/test-scripts"
            echo "  2. Fix any reported issues"
            echo ""
            exit 1
          fi

  quality-gates:
    runs-on: ubuntu-latest
    needs: [build-and-test, automation-scripts, script-tests]
    if: always()

    steps:
      - name: Check build status
        run: |
          if [ "${{ needs.build-and-test.result }}" != "success" ]; then
            echo "Build and test job failed"
            exit 1
          fi
          if [ "${{ needs.automation-scripts.result }}" != "success" ]; then
            echo "Automation scripts job failed"
            exit 1
          fi
          if [ "${{ needs.script-tests.result }}" != "success" ]; then
            echo "Script tests job failed"
            exit 1
          fi
          echo "All quality gates passed"

  automation-scripts:
    runs-on: ubuntu-latest
    name: Automation Scripts CI

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "npm"
          cache-dependency-path: ".github/scripts/package*.json"

      - name: Install dependencies
        run: |
          cd .github/scripts
          npm ci --no-audit --no-fund

      - name: Check for ANSI escape sequences
        run: |
          echo "Checking for ANSI escape sequences in source files..."
          if find . -name "*.js" -o -name "*.cs" -o -name "*.md" | \
             grep -v node_modules | grep -v bin | grep -v obj | \
             xargs grep -l $'\033\[' 2>/dev/null; then
            echo "❌ ANSI escape sequences found in source files!"
            echo "   This indicates formatting issues or terminal output in code."
            find . -name "*.js" -o -name "*.cs" -o -name "*.md" | \
               grep -v node_modules | grep -v bin | grep -v obj | \
               xargs grep -n $'\033\[' 2>/dev/null || true
            exit 1
          fi
          echo "✅ No ANSI escape sequences detected"

      - name: Validate dependencies and security
        run: |
          cd .github/scripts
          echo "Checking package-lock.json synchronization..."
          if ! npm ci --dry-run > /dev/null 2>&1; then
            echo "❌ package-lock.json is out of sync with package.json"
            echo "   Run 'npm install' and commit the updated package-lock.json"
            exit 1
          fi
          echo "✅ Dependencies are synchronized"

          echo "Running security audit..."
          npm audit --audit-level=high
          echo "✅ No high-severity vulnerabilities found"

      - name: Validate YAML files and lint automation scripts
        env:
          DEBUG: "1"
        run: |
          cd .github/scripts
          echo "Running lint-with-annotations.sh with enhanced error handling..."
          ./lint-with-annotations.sh

      - name: Run actionlint
        uses: reviewdog/action-actionlint@v1
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          level: error
          fail_level: error

      - name: Validate package.json structure
        run: |
          cd .github/scripts
          echo "Validating package.json structure..."

          # Check required fields exist
          if ! jq -e '.name' package.json > /dev/null; then
            echo "❌ package.json missing 'name' field"
            exit 1
          fi

          if ! jq -e '.scripts.lint' package.json > /dev/null; then
            echo "❌ package.json missing 'lint' script"
            exit 1
          fi

          if ! jq -e '.scripts.test' package.json > /dev/null; then
            echo "❌ package.json missing 'test' script"
            exit 1
          fi

          echo "✅ package.json structure is valid"

          echo "Checking for outdated packages..."
          npm outdated || true

      - name: Test task parsing with error handling
        run: |
          cd .github/scripts
          echo "Testing task parsing functionality with enhanced error handling..."

          # Check if task file exists
          if [ ! -f "../../MSP430_EMULATOR_TASKS.md" ]; then
            echo "❌ MSP430_EMULATOR_TASKS.md not found"
            exit 1
          fi

          # Test parsing with timeout and proper error handling
          if timeout 60s node parse-tasks.js ../../MSP430_EMULATOR_TASKS.md > \
             /tmp/parse-test.json 2>/tmp/parse-error.log; then
            echo "✅ Parse test passed"
            
            # Validate JSON output
            if jq empty /tmp/parse-test.json 2>/dev/null; then
              echo "✅ Generated valid JSON"
              
              # Check if output has reasonable size (not empty, not too large)
              local size
              size=$(wc -c < /tmp/parse-test.json)
              if [ "$size" -lt 10 ]; then
                echo "❌ Generated JSON seems too small ($size bytes)"
                exit 1
              elif [ "$size" -gt 1048576 ]; then  # 1MB
                echo "❌ Generated JSON seems too large ($size bytes)"
                exit 1
              fi
              echo "✅ Generated JSON has reasonable size ($size bytes)"
            else
              echo "❌ Generated invalid JSON"
              cat /tmp/parse-test.json
              exit 1
            fi
          else
            echo "❌ Parse test failed or timed out"
            if [ -f /tmp/parse-error.log ]; then
              echo "Error log:"
              cat /tmp/parse-error.log
            fi
            exit 1
          fi

      - name: Test dry-run functionality with error handling
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          cd .github/scripts
          echo "Testing dry-run functionality with enhanced error handling..."

          # Test dry-run with timeout and proper error handling
          if timeout 60s node dry-run.js ../../MSP430_EMULATOR_TASKS.md > \
             /tmp/dry-run-test.txt 2>/tmp/dry-run-error.log; then
            echo "✅ Dry-run test passed"
            
            # Check if output has reasonable content
            if [ -s /tmp/dry-run-test.txt ]; then
              local lines
              lines=$(wc -l < /tmp/dry-run-test.txt)
              echo "✅ Dry-run generated output ($lines lines)"
            else
              echo "❌ Dry-run generated no output"
              exit 1
            fi
          else
            echo "❌ Dry-run test failed or timed out"
            if [ -f /tmp/dry-run-error.log ]; then
              echo "Error log:"
              cat /tmp/dry-run-error.log
            fi
            exit 1
          fi

      - name: Upload test artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: automation-test-results
          path: |
            /tmp/parse-test.json
            /tmp/dry-run-test.txt
            /tmp/parse-error.log
            /tmp/dry-run-error.log
          retention-days: 7
